[{"path":"https://lukakoning.github.io/fabricQueryR/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"MIT License","title":"MIT License","text":"Copyright (c) 2025 fabricQueryR authors Permission hereby granted, free charge, person obtaining copy software associated documentation files (“Software”), deal Software without restriction, including without limitation rights use, copy, modify, merge, publish, distribute, sublicense, /sell copies Software, permit persons Software furnished , subject following conditions: copyright notice permission notice shall included copies substantial portions Software. SOFTWARE PROVIDED “”, WITHOUT WARRANTY KIND, EXPRESS IMPLIED, INCLUDING LIMITED WARRANTIES MERCHANTABILITY, FITNESS PARTICULAR PURPOSE NONINFRINGEMENT. EVENT SHALL AUTHORS COPYRIGHT HOLDERS LIABLE CLAIM, DAMAGES LIABILITY, WHETHER ACTION CONTRACT, TORT OTHERWISE, ARISING , CONNECTION SOFTWARE USE DEALINGS SOFTWARE.","code":""},{"path":"https://lukakoning.github.io/fabricQueryR/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Luka Koning. Author, maintainer, copyright holder. Kennispunt Twente. Funder.","code":""},{"path":"https://lukakoning.github.io/fabricQueryR/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Koning L (2025). fabricQueryR: Query Data 'Microsoft Fabric'. R package version 0.1.1.9000, https://github.com/kennispunttwente/fabricQueryR.","code":"@Manual{,   title = {fabricQueryR: Query Data in 'Microsoft Fabric'},   author = {Luka Koning},   year = {2025},   note = {R package version 0.1.1.9000},   url = {https://github.com/kennispunttwente/fabricQueryR}, }"},{"path":"https://lukakoning.github.io/fabricQueryR/index.html","id":"fabricqueryr","dir":"","previous_headings":"","what":"Query Data in Microsoft Fabric","title":"Query Data in Microsoft Fabric","text":"‘fabricQueryR’ R package helps query data Microsoft Fabric R. comes four methods help get Microsoft Fabric data R: Create connection SQL endpoint (e.g., Lakehouse Data Warehouse item): fabric_sql_connect(). results ‘DBI’ connection object can execute SQL queries , /use ‘DBI’-compatible packages like ‘dbplyr’. Execute DAX query Fabric/Power Bi Semantic Model item: fabric_pbi_dax_query(). , can run DAX queries Fabric/Power Bi dataset get results ‘tibble’ dataframe. Read Delta table Fabric Lakehouse item: fabric_onelake_read_delta_table(). function downloads underlying Parquet files Delta table stored OneLake (ADLS Gen2) returns data ‘tibble’ dataframe. Execute Livy API query: fabric_livy_query(). , can remotely execute Spark/Spark SQL/SparkR/PySpark code Microsoft Fabric get list results local R session.","code":""},{"path":"https://lukakoning.github.io/fabricQueryR/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Query Data in Microsoft Fabric","text":"can install development version ‘fabricQueryR’ like : , install latest release CRAN:","code":"if (!requireNamespace(\"remotes\", quietly = TRUE)) {   install.packages(\"remotes\") }  remotes::install_github(\"kennispunttwente/fabricQueryR\") install.packages(\"fabricQueryR\")"},{"path":"https://lukakoning.github.io/fabricQueryR/index.html","id":"usage","dir":"","previous_headings":"","what":"Usage","title":"Query Data in Microsoft Fabric","text":"See reference full documentation functions. code snippet showing use four methods get data Fabric R:","code":"# First find your 'tenant' ID & 'client' ID (app registration) in Azure/Entra # You may be able to use the default Azure CLI app id; #   this will be automatically used if you do not set 'FABRICQUERYR_CLIENT_ID' # The AzureAuth package is used to acquire tokens; you may be redirected #   to a browser window to sign in the first time  library(fabricQueryR)  # Sys.setenv(FABRICQUERYR_TENANT_ID = \"...\") # Sys.setenv(FABRICQUERYR_CLIENT_ID = \"...\")  # SQL connection to Data Warehouse or Lakehouse --------------------------------  # Find your SQL connection string in Fabric by going to a Lakehouse or Data #   Warehouse item; then Settings -> SQL analytics endpoint # Ensure that the account/principal you authenticate with has access to #   the workspace   # Get connection con <- fabric_sql_connect(   server = \"2gxz...4qiy.datawarehouse.fabric.microsoft.com\" )  # List databases DBI::dbGetQuery(con, \"SELECT name FROM sys.databases\")  # List tables in the current database DBI::dbGetQuery(   con,   \"   SELECT TABLE_SCHEMA, TABLE_NAME   FROM INFORMATION_SCHEMA.TABLES   WHERE TABLE_TYPE = 'BASE TABLE'   \" )  # Read 'Customers' table df_sql <- DBI::dbReadTable(con, \"Customers\")  # Close connection DBI::dbDisconnect(con)   # Table from Lakehouse via OneLake data access ---------------------------------  # Ensure that the account/principal you authenticate with has access via #   being part of the workspace, or via Lakehouse -> Manage OneLake data access  df_onelake <- fabric_onelake_read_delta_table(   table_path = \"Customers\",   workspace_name = \"ExampleWorkspace\",   lakehouse_name = \"Lakehouse.Lakehouse\", )   # DAX query against Semantic Model ---------------------------------------------  # Ensure that the account you use to authenticate has access to the workspace, #   or that you have been granted 'Build' permissions on the dataset (via share)  df_dax <- fabric_pbi_dax_query(   connstr = paste0(     \"Data Source=powerbi://api.powerbi.com/v1.0/myorg/\",     \"ExampleWorkspace;Initial Catalog=test data 1;\"   ),   dax = \"EVALUATE TOPN(100000, 'Sheet1')\" )   # Livy query to execute Spark code ---------------------------------------------  # Find your session URL in Fabric by going to a 'Lakehouse' item, #   then go to 'Settings' -> 'Livy Endpoint' -> 'Session job connection string' sess_url <- \"https://api.fabric.microsoft.com/v1/workspaces/.../lakehouses/.../livyapi/...\"  # Run a Livy SparkR query livy_sparkr_result <- fabric_livy_query(   livy_url = sess_url,   kind = \"sparkr\",   code = \"print(1+2)\" )  # (See example in `?fabric_livy_query` for how to get data from the Lakehouse to R with Spark)"},{"path":"https://lukakoning.github.io/fabricQueryR/index.html","id":"background","dir":"","previous_headings":"","what":"Background","title":"Query Data in Microsoft Fabric","text":"Microsoft Fabric new data platform Microsoft combines various data services, including data warehousing, data lakes, business intelligence. built top Azure Data Services integrates Power BI analytics reporting. Microsoft actively promoting Fabric next-generation data platform organizations using Microsoft Azure Power BI. organization started working Microsoft Fabric, found loading data R Fabric yet straightforward, took effort get working. help others situation, decided share functions created make easier.","code":""},{"path":"https://lukakoning.github.io/fabricQueryR/reference/fabricQueryR-package.html","id":null,"dir":"Reference","previous_headings":"","what":"fabricQueryR: Query Data in 'Microsoft Fabric' — fabricQueryR-package","title":"fabricQueryR: Query Data in 'Microsoft Fabric' — fabricQueryR-package","text":"Query data hosted 'Microsoft Fabric'. Provides helpers open 'DBI' connections 'SQL' endpoints 'Lakehouse' 'Data Warehouse' items; submit 'Data Analysis Expressions' ('DAX') queries semantic model datasets 'Microsoft Fabric' 'Power BI'; read 'Delta Lake' tables stored 'OneLake' ('Azure Data Lake Storage Gen2'); execute 'Spark' code via 'Fabric Livy API'.","code":""},{"path":[]},{"path":"https://lukakoning.github.io/fabricQueryR/reference/fabricQueryR-package.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"fabricQueryR: Query Data in 'Microsoft Fabric' — fabricQueryR-package","text":"Maintainer: Luka Koning l.koning@kennispunttwente.nl [copyright holder] contributors: Kennispunt Twente info@kennispunttwente.nl [funder]","code":""},{"path":"https://lukakoning.github.io/fabricQueryR/reference/fabric_livy_query.html","id":null,"dir":"Reference","previous_headings":"","what":"Run a Livy API query (Spark code) in Microsoft Fabric — fabric_livy_query","title":"Run a Livy API query (Spark code) in Microsoft Fabric — fabric_livy_query","text":"High-level helper creates Livy session Microsoft Fabric, waits become idle, submits statement Spark code execution, retrieves result, closes session.","code":""},{"path":"https://lukakoning.github.io/fabricQueryR/reference/fabric_livy_query.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Run a Livy API query (Spark code) in Microsoft Fabric — fabric_livy_query","text":"","code":"fabric_livy_query(   livy_url,   code,   kind = c(\"spark\", \"pyspark\", \"sparkr\", \"sql\"),   tenant_id = Sys.getenv(\"FABRICQUERYR_TENANT_ID\"),   client_id = Sys.getenv(\"FABRICQUERYR_CLIENT_ID\", unset =     \"04b07795-8ddb-461a-bbee-02f9e1bf7b46\"),   access_token = NULL,   environment_id = NULL,   conf = NULL,   verbose = TRUE,   poll_interval = 2L,   timeout = 600L )"},{"path":"https://lukakoning.github.io/fabricQueryR/reference/fabric_livy_query.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Run a Livy API query (Spark code) in Microsoft Fabric — fabric_livy_query","text":"livy_url Character. Livy session job connection string, e.g. \"https://api.fabric.microsoft.com/v1/workspaces/.../lakehouses/.../livyapi/versions/2023-12-01/sessions\" (see details). code Character. Code run Livy session. kind Character. One \"spark\", \"pyspark\", \"sparkr\", \"sql\". Indicates type Spark code submitted evaluation. tenant_id Microsoft Azure tenant ID. Defaults Sys.getenv(\"FABRICQUERYR_TENANT_ID\") missing. client_id Microsoft Azure application (client) ID used authenticate. Defaults Sys.getenv(\"FABRICQUERYR_CLIENT_ID\"). may able use Azure CLI app id \"04b07795-8ddb-461a-bbee-02f9e1bf7b46\", may want make app registration tenant better control. access_token Optional character. supplied, use bearer token instead acquiring new one via {AzureAuth}. environment_id Optional character. Fabric Environment (pool) ID use session. NULL (default), default environment user used. conf Optional list. Spark configuration settings apply session. verbose Logical. Emit progress via {cli}. Default TRUE. poll_interval Integer. Polling interval seconds waiting session/statement readiness. timeout Integer. Timeout seconds waiting session/statement readiness.","code":""},{"path":"https://lukakoning.github.io/fabricQueryR/reference/fabric_livy_query.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Run a Livy API query (Spark code) in Microsoft Fabric — fabric_livy_query","text":"list statement details results. list contains: id: Statement ID. state: Final statement state (\"available\"). started_local: Local timestamp statement started running. completed_local: Local timestamp statement completed. duration_sec: Duration seconds (local). output: list raw output details: status: Output status (e.g., \"ok\"). execution_count: Execution count (applicable). number statements executed session. data: Raw data list MIME types keys (e.g. \"text/plain\", \"application/json\"). parsed: Parsed output, possible. may data frame (tibble) output JSON tabular data, character vector plain text. May NULL parsing possible. url: URL statement resource Livy API.","code":""},{"path":"https://lukakoning.github.io/fabricQueryR/reference/fabric_livy_query.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Run a Livy API query (Spark code) in Microsoft Fabric — fabric_livy_query","text":"Microsoft Fabric, can find copy Livy session URL going 'Lakehouse' item, go 'Settings' -> 'Livy Endpoint' -> 'Session job connection string'. default request token https://api.fabric.microsoft.com/.default. AzureAuth used acquire token. wary caching behavior; may want call AzureAuth::clean_token_directory() clear cached tokens run issues","code":""},{"path":[]},{"path":"https://lukakoning.github.io/fabricQueryR/reference/fabric_livy_query.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Run a Livy API query (Spark code) in Microsoft Fabric — fabric_livy_query","text":"","code":"# Find your session URL in Fabric by going to a 'Lakehouse' item, #   then go to 'Settings' -> 'Livy Endpoint' -> 'Session job connection string' sess_url <- \"https://api.fabric.microsoft.com/v1/workspaces/.../lakehouses/.../livyapi/...\"  # Livy API can run SQL, SparkR, PySpark, & Spark # Below are examples of 1) SQL & 2) SparkR usage  # Example is not executed since it requires configured credentials for Fabric if (FALSE) { # \\dontrun{ ## 1 Livy & SQL  # Here we run SQL remotely in Microsoft Fabric with Spark, to get data to local R # Since Livy API cannot directly give back a proper DF, we build it from returned schema & matrix  # Run Livy SQL query livy_sql_result <- fabric_livy_query(   livy_url = sess_url,   kind = \"sql\",   code = \"SELECT * FROM Patienten LIMIT 1000\",   tenant_id = Sys.getenv(\"FABRICQUERYR_TENANT_ID\"),   client_id = Sys.getenv(\"FABRICQUERYR_CLIENT_ID\") )  # '$schema$fields' contains column info, & '$data' contains data as matrix without column names payload <- livy_sql_result$output$data[[\"application/json\"]] schema  <- as_tibble(payload$schema$fields) # has columns: name, type, nullable col_nms <- schema$name  # Build dataframe (tibble) from the Livy result df_livy_sql <- payload$data |>   as_tibble(.name_repair = \"minimal\") |>   set_names(col_nms) |>   mutate(     # cast by schema$type (add more cases if your schema includes them)     across(all_of(schema$name[schema$type == \"long\"]),    readr::parse_integer),     across(all_of(schema$name[schema$type == \"double\"]),  readr::parse_double),     across(all_of(schema$name[schema$type == \"boolean\"]), readr::parse_logical),     across(all_of(schema$name[schema$type == \"string\"]),  as.character)   )  ## 2 Livy & SparkR  # Here we run R code remotely in Microsoft Fabric with SparkR, to get data to local R # Since Livy API cannot directly give back a proper DF, we encode/decode B64 in SparkR/local R  # Run Livy SparkR query livy_sparkr_result <- fabric_livy_query(   livy_url = sess_url,   kind = \"sparkr\",   code = paste(     # Obtain data in remote R (SparkR)     'library(SparkR); library(base64enc)',     'df <- sql(\"SELECT * FROM Patienten\") |> limit(1000L) |> collect()',      # serialize -> gzip -> base64     'r_raw <- serialize(df, connection = NULL)',     'raw_gz <- memCompress(r_raw, type = \"gzip\")',     'b64 <- base64enc::base64encode(raw_gz)',      # output marked B64 string     'cat(\"<<B64RDS>>\", b64, \"<<END>>\", sep = \"\")',     sep = \"\\n\"   ),   tenant_id = Sys.getenv(\"FABRICQUERYR_TENANT_ID\"),   client_id = Sys.getenv(\"FABRICQUERYR_CLIENT_ID\") )  # Extract marked B64 string from Livy output txt <- livy_sparkr_result$output$data$`text/plain` b64 <- sub('.*<<B64RDS>>', '', txt) b64 <- sub('<<END>>.*', '', b64)  # Decode to dataframe raw_gz <- base64enc::base64decode(b64) r_raw  <- memDecompress(raw_gz, type = \"gzip\") df_livy_sparkr <- unserialize(r_raw) } # }"},{"path":"https://lukakoning.github.io/fabricQueryR/reference/fabric_onelake_read_delta_table.html","id":null,"dir":"Reference","previous_headings":"","what":"Read a Microsoft Fabric/OneLake Delta table (ADLS Gen2) — fabric_onelake_read_delta_table","title":"Read a Microsoft Fabric/OneLake Delta table (ADLS Gen2) — fabric_onelake_read_delta_table","text":"Authenticates OneLake (ADLS Gen2), resolves table's _delta_log determine current active Parquet parts, downloads parts local staging directory, returns result tibble.","code":""},{"path":"https://lukakoning.github.io/fabricQueryR/reference/fabric_onelake_read_delta_table.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Read a Microsoft Fabric/OneLake Delta table (ADLS Gen2) — fabric_onelake_read_delta_table","text":"","code":"fabric_onelake_read_delta_table(   table_path,   workspace_name,   lakehouse_name,   tenant_id = Sys.getenv(\"FABRICQUERYR_TENANT_ID\"),   client_id = Sys.getenv(\"FABRICQUERYR_CLIENT_ID\", unset =     \"04b07795-8ddb-461a-bbee-02f9e1bf7b46\"),   dest_dir = NULL,   verbose = TRUE,   dfs_base = \"https://onelake.dfs.fabric.microsoft.com\" )"},{"path":"https://lukakoning.github.io/fabricQueryR/reference/fabric_onelake_read_delta_table.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Read a Microsoft Fabric/OneLake Delta table (ADLS Gen2) — fabric_onelake_read_delta_table","text":"table_path Character. Table name nested path (e.g. \"Patienten\" \"Patienten/patienten_hash\"). last path segment used table directory Tables/. workspace_name Character. Fabric workspace display name GUID (ADLS filesystem/container name). lakehouse_name Character. Lakehouse item name, without .Lakehouse suffix (e.g. \"Lakehouse\" \"Lakehouse.Lakehouse\"). tenant_id Character. Entra ID (Azure AD) tenant GUID. Defaults Sys.getenv(\"FABRICQUERYR_TENANT_ID\") missing. client_id Character. App registration (client) ID. Defaults Sys.getenv(\"FABRICQUERYR_CLIENT_ID\"), falling back Azure CLI app id \"04b07795-8ddb-461a-bbee-02f9e1bf7b46\" set. dest_dir Character NULL. Local staging directory Parquet parts. NULL (default), temp dir used cleaned exit. verbose Logical. Print progress messages via {cli}. Default TRUE. dfs_base Character. OneLake DFS endpoint. Default \"https://onelake.dfs.fabric.microsoft.com\".","code":""},{"path":"https://lukakoning.github.io/fabricQueryR/reference/fabric_onelake_read_delta_table.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Read a Microsoft Fabric/OneLake Delta table (ADLS Gen2) — fabric_onelake_read_delta_table","text":"tibble table's current rows (0 rows table empty).","code":""},{"path":"https://lukakoning.github.io/fabricQueryR/reference/fabric_onelake_read_delta_table.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Read a Microsoft Fabric/OneLake Delta table (ADLS Gen2) — fabric_onelake_read_delta_table","text":"Microsoft Fabric, OneLake exposes workspace ADLS Gen2 filesystem. Within Lakehouse item, Delta tables stored Tables/<table> _delta_log/ directory tracks commit state. helper replays JSON commits avoid double-counting compacted/removed files. Ensure account/principal authenticate access via Lakehouse -> Manage OneLake data access (member workspace). AzureAuth used acquire token. wary caching behavior; may want call AzureAuth::clean_token_directory() clear cached tokens run issues","code":""},{"path":"https://lukakoning.github.io/fabricQueryR/reference/fabric_onelake_read_delta_table.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Read a Microsoft Fabric/OneLake Delta table (ADLS Gen2) — fabric_onelake_read_delta_table","text":"","code":"# Example is not executed since it requires configured credentials for Fabric if (FALSE) { # \\dontrun{ df <- fabric_onelake_read_delta_table(   table_path     = \"Patients/PatientInfo\",   workspace_name = \"PatientsWorkspace\",   lakehouse_name = \"Lakehouse.Lakehouse\",   tenant_id      = Sys.getenv(\"FABRICQUERYR_TENANT_ID\"),   client_id      = Sys.getenv(\"FABRICQUERYR_CLIENT_ID\") ) dplyr::glimpse(df) } # }"},{"path":"https://lukakoning.github.io/fabricQueryR/reference/fabric_pbi_dax_query.html","id":null,"dir":"Reference","previous_headings":"","what":"Query a Microsoft Fabric/Power Bi semantic model with DAX — fabric_pbi_dax_query","title":"Query a Microsoft Fabric/Power Bi semantic model with DAX — fabric_pbi_dax_query","text":"High-level helper authenticates Azure AD, resolves workspace & dataset Power BI (Microsoft Fabric) XMLA/connection string, executes DAX statement via Power BI REST API, returns tibble resulting data.","code":""},{"path":"https://lukakoning.github.io/fabricQueryR/reference/fabric_pbi_dax_query.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Query a Microsoft Fabric/Power Bi semantic model with DAX — fabric_pbi_dax_query","text":"","code":"fabric_pbi_dax_query(   connstr,   dax,   tenant_id = Sys.getenv(\"FABRICQUERYR_TENANT_ID\"),   client_id = Sys.getenv(\"FABRICQUERYR_CLIENT_ID\", unset =     \"04b07795-8ddb-461a-bbee-02f9e1bf7b46\"),   include_nulls = TRUE,   api_base = \"https://api.powerbi.com/v1.0/myorg\" )"},{"path":"https://lukakoning.github.io/fabricQueryR/reference/fabric_pbi_dax_query.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Query a Microsoft Fabric/Power Bi semantic model with DAX — fabric_pbi_dax_query","text":"connstr Character. Power BI connection string, e.g. \"Data Source=powerbi://api.powerbi.com/v1.0/myorg/Workspace;Initial Catalog=Dataset;\". function accepts either Data Source= Initial Catalog= parts, bare powerbi://... data source plus Dataset=/Catalog=/Initial Catalog= key (see details). dax Character scalar valid DAX query (see example). tenant_id Microsoft Azure tenant ID. Defaults Sys.getenv(\"FABRICQUERYR_TENANT_ID\") missing. client_id Microsoft Azure application (client) ID used authenticate. Defaults Sys.getenv(\"FABRICQUERYR_CLIENT_ID\"). may able use Azure CLI app id \"04b07795-8ddb-461a-bbee-02f9e1bf7b46\", may want make app registration tenant better control. include_nulls Logical; pass-REST serializer setting. Defaults TRUE. TRUE, null values included response; FALSE, omitted. api_base API base URL. Defaults \"https://api.powerbi.com/v1.0/myorg\". 'myorg' appropriate use cases necessarily need changed.","code":""},{"path":"https://lukakoning.github.io/fabricQueryR/reference/fabric_pbi_dax_query.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Query a Microsoft Fabric/Power Bi semantic model with DAX — fabric_pbi_dax_query","text":"tibble query result (0 rows DAX query returned rows).","code":""},{"path":"https://lukakoning.github.io/fabricQueryR/reference/fabric_pbi_dax_query.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Query a Microsoft Fabric/Power Bi semantic model with DAX — fabric_pbi_dax_query","text":"Microsoft Fabric/Power BI, can find copy connection string going 'Semantic model' item, go 'File' -> 'Settings' -> 'Server settings'. Ensure account use authenticate access workspace, granted 'Build' permissions dataset (via sharing). AzureAuth used acquire token. wary caching behavior; may want call AzureAuth::clean_token_directory() clear cached tokens run issues","code":""},{"path":"https://lukakoning.github.io/fabricQueryR/reference/fabric_pbi_dax_query.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Query a Microsoft Fabric/Power Bi semantic model with DAX — fabric_pbi_dax_query","text":"","code":"# Example is not executed since it requires configured credentials for Fabric if (FALSE) { # \\dontrun{ conn <- \"Data Source=powerbi://api.powerbi.com/v1.0/myorg/My Workspace;Initial Catalog=SalesModel;\" df <- fabric_pbi_dax_query(   connstr = conn,   dax = \"EVALUATE TOPN(1000, 'Customers')\",   tenant_id = Sys.getenv(\"FABRICQUERYR_TENANT_ID\"),   client_id = Sys.getenv(\"FABRICQUERYR_CLIENT_ID\") ) dplyr::glimpse(df) } # }"},{"path":"https://lukakoning.github.io/fabricQueryR/reference/fabric_sql_connect.html","id":null,"dir":"Reference","previous_headings":"","what":"Connect to a Microsoft Fabric SQL endpoint — fabric_sql_connect","title":"Connect to a Microsoft Fabric SQL endpoint — fabric_sql_connect","text":"Opens DBI/ODBC connection Microsoft Fabric Data Warehouse Lakehouse SQL endpoint, authenticating Azure AD (MSAL v2) passing access token ODBC driver.","code":""},{"path":"https://lukakoning.github.io/fabricQueryR/reference/fabric_sql_connect.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Connect to a Microsoft Fabric SQL endpoint — fabric_sql_connect","text":"","code":"fabric_sql_connect(   server,   database = \"Lakehouse\",   tenant_id = Sys.getenv(\"FABRICQUERYR_TENANT_ID\"),   client_id = Sys.getenv(\"FABRICQUERYR_CLIENT_ID\", unset =     \"04b07795-8ddb-461a-bbee-02f9e1bf7b46\"),   access_token = NULL,   odbc_driver = getOption(\"fabricqueryr.sql.driver\", \"ODBC Driver 18 for SQL Server\"),   port = 1433L,   encrypt = \"yes\",   trust_server_certificate = \"no\",   timeout = 30L,   verbose = TRUE,   ... )"},{"path":"https://lukakoning.github.io/fabricQueryR/reference/fabric_sql_connect.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Connect to a Microsoft Fabric SQL endpoint — fabric_sql_connect","text":"server Character. Microsoft Fabric SQL connection string Server=... string (see details). database Character. Database name. Defaults \"Lakehouse\". tenant_id Character. Entra ID (AAD) tenant GUID. Defaults Sys.getenv(\"FABRICQUERYR_TENANT_ID\"). client_id Character. App registration (client) ID. Defaults Sys.getenv(\"FABRICQUERYR_CLIENT_ID\"), falling back Azure CLI app id \"04b07795-8ddb-461a-bbee-02f9e1bf7b46\" unset. access_token Optional character. supplied, use bearer token instead acquiring new one via {AzureAuth}. odbc_driver Character. ODBC driver name. Defaults getOption(\"fabricqueryr.sql.driver\", \"ODBC Driver 18 SQL Server\"). port Integer. TCP port (default 1433). encrypt, trust_server_certificate Character flags passed ODBC. Defaults \"yes\" \"\", respectively. timeout Integer. Login/connect timeout seconds. Default 30. verbose Logical. Emit progress via {cli}. Default TRUE. ... Additional arguments forwarded DBI::dbConnect().","code":""},{"path":"https://lukakoning.github.io/fabricQueryR/reference/fabric_sql_connect.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Connect to a Microsoft Fabric SQL endpoint — fabric_sql_connect","text":"live DBIConnection object.","code":""},{"path":"https://lukakoning.github.io/fabricQueryR/reference/fabric_sql_connect.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Connect to a Microsoft Fabric SQL endpoint — fabric_sql_connect","text":"server Microsoft Fabric SQL connection string, e.g. \"xxxx.datawarehouse.fabric.microsoft.com\". can find going Lakehouse Data Warehouse item, Settings -> SQL analytics endpoint -> SQL connection string. may also pass DSN-less Server=... string; normalized. default request token https://database.windows.net/.default. AzureAuth used acquire token. wary caching behavior; may want call AzureAuth::clean_token_directory() clear cached tokens run issues","code":""},{"path":"https://lukakoning.github.io/fabricQueryR/reference/fabric_sql_connect.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Connect to a Microsoft Fabric SQL endpoint — fabric_sql_connect","text":"","code":"# Example is not executed since it requires configured credentials for Fabric if (FALSE) { # \\dontrun{ con <- fabric_sql_connect(   server    = \"2gxz...qiy.datawarehouse.fabric.microsoft.com\",   database  = \"Lakehouse\",   tenant_id = Sys.getenv(\"FABRICQUERYR_TENANT_ID\"),   client_id = Sys.getenv(\"FABRICQUERYR_CLIENT_ID\") )  # List databases DBI::dbGetQuery(con, \"SELECT name FROM sys.databases\")  # List tables DBI::dbGetQuery(con, \"  SELECT TABLE_SCHEMA, TABLE_NAME  FROM INFORMATION_SCHEMA.TABLES  WHERE TABLE_TYPE = 'BASE TABLE' \")  # Get a table df <- DBI::dbReadTable(con, \"Customers\") dplyr::glimpse(df)  DBI::dbDisconnect(con) } # }"},{"path":"https://lukakoning.github.io/fabricQueryR/reference/fabric_sql_query.html","id":null,"dir":"Reference","previous_headings":"","what":"Run a SQL query against a Microsoft Fabric SQL endpoint (opening & closing connection) — fabric_sql_query","title":"Run a SQL query against a Microsoft Fabric SQL endpoint (opening & closing connection) — fabric_sql_query","text":"Convenience wrapper opens connection fabric_sql_connect(), executes sql, returns tibble. connection closed exit.","code":""},{"path":"https://lukakoning.github.io/fabricQueryR/reference/fabric_sql_query.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Run a SQL query against a Microsoft Fabric SQL endpoint (opening & closing connection) — fabric_sql_query","text":"","code":"fabric_sql_query(   server,   sql,   database = \"Lakehouse\",   tenant_id = Sys.getenv(\"FABRICQUERYR_TENANT_ID\"),   client_id = Sys.getenv(\"FABRICQUERYR_CLIENT_ID\", unset =     \"04b07795-8ddb-461a-bbee-02f9e1bf7b46\"),   access_token = NULL,   odbc_driver = getOption(\"fabricqueryr.sql.driver\", \"ODBC Driver 18 for SQL Server\"),   port = 1433L,   encrypt = \"yes\",   trust_server_certificate = \"no\",   timeout = 30L,   verbose = TRUE,   ... )"},{"path":"https://lukakoning.github.io/fabricQueryR/reference/fabric_sql_query.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Run a SQL query against a Microsoft Fabric SQL endpoint (opening & closing connection) — fabric_sql_query","text":"server Character. Microsoft Fabric SQL connection string Server=... string (see details). sql Character scalar. SQL run. database Character. Database name. Defaults \"Lakehouse\". tenant_id Character. Entra ID (AAD) tenant GUID. Defaults Sys.getenv(\"FABRICQUERYR_TENANT_ID\"). client_id Character. App registration (client) ID. Defaults Sys.getenv(\"FABRICQUERYR_CLIENT_ID\"), falling back Azure CLI app id \"04b07795-8ddb-461a-bbee-02f9e1bf7b46\" unset. access_token Optional character. supplied, use bearer token instead acquiring new one via {AzureAuth}. odbc_driver Character. ODBC driver name. Defaults getOption(\"fabricqueryr.sql.driver\", \"ODBC Driver 18 SQL Server\"). port Integer. TCP port (default 1433). encrypt, trust_server_certificate Character flags passed ODBC. Defaults \"yes\" \"\", respectively. timeout Integer. Login/connect timeout seconds. Default 30. verbose Logical. Emit progress via {cli}. Default TRUE. ... Additional arguments forwarded DBI::dbConnect().","code":""},{"path":"https://lukakoning.github.io/fabricQueryR/reference/fabric_sql_query.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Run a SQL query against a Microsoft Fabric SQL endpoint (opening & closing connection) — fabric_sql_query","text":"tibble query results (0 rows none).","code":""},{"path":"https://lukakoning.github.io/fabricQueryR/reference/fabric_sql_query.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Run a SQL query against a Microsoft Fabric SQL endpoint (opening & closing connection) — fabric_sql_query","text":"","code":"# Example is not executed since it requires configured credentials for Fabric if (FALSE) { # \\dontrun{ df <- fabric_sql_query(   server    = \"2gxz...qiy.datawarehouse.fabric.microsoft.com\",   database  = \"Lakehouse\",   sql       = \"SELECT TOP 100 * FROM sys.objects\",   tenant_id = Sys.getenv(\"FABRICQUERYR_TENANT_ID\"),   client_id = Sys.getenv(\"FABRICQUERYR_CLIENT_ID\") ) dplyr::glimpse(df) } # }"},{"path":[]},{"path":"https://lukakoning.github.io/fabricQueryR/news/index.html","id":"fabricqueryr-011","dir":"Changelog","previous_headings":"","what":"fabricQueryR 0.1.1","title":"fabricQueryR 0.1.1","text":"CRAN release: 2025-09-08 Initial CRAN submission.","code":""}]
